<!DOCTYPE html>
<html lang="en" data-theme="dark">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>DelightMate - Your AI Assistant</title>
    <link href="{{ url_for('static', filename='css/tailwind.css') }}" rel="stylesheet">
    <link href="{{ url_for('static', filename='css/cosmic.css') }}" rel="stylesheet">
    <script src="{{ url_for('static', filename='js/theme.js') }}"></script>
    <style>
        body, html {
            margin: 0;
            padding: 0;
            height: 100%;
            overflow: hidden;
            font-family: 'Arial', sans-serif;
        }
    </style>
</head>
<body style="background-color: var(--bg-color);">
    <div class="cosmos-container">
        {% include 'components/navbar.html' %}
        
        <!-- User Profile -->
        {% if user and user.is_logged_in %}
        <div class="cosmic-user-profile">
            {% if user.picture %}
            <img src="{{ user.picture }}" alt="Profile">
            {% else %}
            <div style="width: 40px; height: 40px; border-radius: 50%; background-color: rgba(64, 190, 255, 0.8); display: flex; justify-content: center; align-items: center; color: white; font-weight: bold;">
                {{ user.name[0] if user.name else 'U' }}
            </div>
            {% endif %}
            <span class="cosmic-user-name">{{ user.name if user.name else 'User' }}</span>
        </div>
        {% endif %}
        
        <!-- Voice Mode Indicator -->
        <!-- <div class="voice-mode">Voice mode</div> -->
        
        <!-- Welcome Message -->
        <!-- <div class="cosmic-welcome" id="welcomeMessage">
            <h2>I'm DelightMate</h2>
            <p>Your digital teammate ready to help with emails, calendar, and more.</p>
            <p class="welcome-hint">Click the microphone to start speaking</p>
        </div> -->

        <!-- Cosmic Sphere -->
        <div class="cosmos-sphere" id="cosmosSphere"></div>
        
        <!-- Status Text -->
        <div class="status-text" id="statusText">Listening...</div>
        
        <!-- Response Container -->
        <div class="cosmic-message-container" id="messageContainer">
            <p class="cosmic-message" id="responseMessage"></p>
        </div>
        
        <!-- Control Buttons -->
        <div class="controls">
            <button class="control-btn">
                <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                    <path d="M22 16.92v3a2 2 0 0 1-2.18 2 19.79 19.79 0 0 1-8.63-3.07 19.5 19.5 0 0 1-6-6 19.79 19.79 0 0 1-3.07-8.67A2 2 0 0 1 4.11 2h3a2 2 0 0 1 2 1.72 12.84 12.84 0 0 0 .7 2.81 2 2 0 0 1-.45 2.11L8.09 9.91a16 16 0 0 0 6 6l1.27-1.27a2 2 0 0 1 2.11-.45 12.84 12.84 0 0 0 2.81.7A2 2 0 0 1 22 16.92z"></path>
                </svg>
            </button>
            <button class="control-btn microphone-btn" id="micButton">
                <svg xmlns="http://www.w3.org/2000/svg" width="28" height="28" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                    <path d="M12 1a3 3 0 0 0-3 3v8a3 3 0 0 0 6 0V4a3 3 0 0 0-3-3z"></path>
                    <path d="M19 10v2a7 7 0 0 1-14 0v-2"></path>
                    <line x1="12" y1="19" x2="12" y2="23"></line>
                    <line x1="8" y1="23" x2="16" y2="23"></line>
                </svg>
            </button>
            <button class="control-btn">
                <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                    <circle cx="11" cy="11" r="8"></circle>
                    <line x1="21" y1="21" x2="16.65" y2="16.65"></line>
                </svg>
            </button>
        </div>

    </div>

    <!-- JavaScript for Cosmic Visualization and Audio Recording -->
    <script>
        document.addEventListener('DOMContentLoaded', function() {
            const cosmosSphere = document.getElementById('cosmosSphere');
            const statusText = document.getElementById('statusText');
            const micButton = document.getElementById('micButton');
            const messageContainer = document.getElementById('messageContainer');
            const responseMessage = document.getElementById('responseMessage');
            
            // Create a dedicated state management object similar to audio_recorder.html
            const AppState = {
                // Recording state
                isRecording: false,
                isProcessing: false,
                pauseDetected: false,
                silenceStart: null,
                hasDetectedAudio: false,
                significantSpeechDetected: false,
                isListening: false,
                
                // Audio resources
                mediaRecorder: null,
                audioChunks: [],
                audioBlob: null,
                audioContext: null,
                analyser: null,
                source: null,
                
                // Timers and tracking
                continuousRecordingTimer: null,
                lastProcessedTime: 0,
                
                // Debug
                logPrefix: 'ðŸŽ¤ VoiceUI:',
                
                // Methods for real-time conversation
                resetAudioState() {
                    this.isRecording = false;
                    this.isProcessing = false;
                    this.pauseDetected = false;
                    this.silenceStart = null;
                    this.hasDetectedAudio = false;
                    this.significantSpeechDetected = false;
                    this.audioChunks = [];
                    console.log(`${this.logPrefix} Audio state reset`);
                },
                
                debug() {
                    console.log(`${this.logPrefix} State: Recording: ${this.isRecording}, Processing: ${this.isProcessing}, Active: ${this.isListening}`);
                }
            };
            
            // Create cosmic particles
            createCosmicParticles();
            
            // Microphone button click handler - modified for real-time conversation
            micButton.addEventListener('click', function() {
                if (AppState.isProcessing) return; // Prevent multiple clicks during processing
                
                if (AppState.isListening) {
                    stopRecording();
                    // Change microphone icon to original icon when stopping
                    micButton.innerHTML = `
                        <svg xmlns="http://www.w3.org/2000/svg" width="28" height="28" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                            <path d="M12 1a3 3 0 0 0-3 3v8a3 3 0 0 0 6 0V4a3 3 0 0 0-3-3z"></path>
                            <path d="M19 10v2a7 7 0 0 1-14 0v-2"></path>
                            <line x1="12" y1="19" x2="12" y2="23"></line>
                            <line x1="8" y1="23" x2="16" y2="23"></line>
                        </svg>
                    `;
                } else {
                    startRecording();
                    // Change microphone icon to stop icon when recording
                    micButton.innerHTML = `
                        <svg xmlns="http://www.w3.org/2000/svg" width="28" height="28" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                            <rect x="6" y="6" width="12" height="12" rx="2" ry="2"></rect>
                        </svg>
                    `;
                    micButton.classList.add('recording');
                }
            });
            
            // Configuration settings for voice assistant
            const settings = {
                silenceThreshold: 15,  // Volume level below which is considered silence
                significantAudioThreshold: 25, // Threshold for significant speech detection
                pauseThreshold: 1.5,   // Seconds of silence before processing
                minAudioSize: 1000,    // Minimum audio blob size to process
                continuousMode: true,  // Whether to automatically resume recording
                echoCancel: true,      // Whether to use echo cancellation
                noiseSuppress: true    // Whether to use noise suppression
            };

            // Initialize audio recording capabilities with continuous mode
            async function startRecording() {
                if (AppState.isRecording) {
                    console.log(`${AppState.logPrefix} Already recording, not starting again`);
                    return;
                }
                
                console.log(`${AppState.logPrefix} Starting recording...`);
                AppState.debug();
                
                // Reset state variables
                AppState.isRecording = true;
                AppState.isListening = true;
                AppState.pauseDetected = false;
                AppState.silenceStart = null;
                AppState.hasDetectedAudio = false;
                AppState.significantSpeechDetected = false;
                AppState.audioChunks = [];
                
                try {
                    // Request microphone access with constraints for better quality
                    const constraints = {
                        audio: {
                            echoCancellation: settings.echoCancel,
                            noiseSuppression: settings.noiseSuppress,
                            autoGainControl: true
                        }
                    };
                    
                    statusText.textContent = "Accessing microphone...";
                    const stream = await navigator.mediaDevices.getUserMedia(constraints);
                    
                    // Start animation
                    startListening();
                    
                    // Initialize audio context for visualization and volume monitoring
                    AppState.audioContext = new (window.AudioContext || window.webkitAudioContext)();
                    AppState.analyser = AppState.audioContext.createAnalyser();
                    AppState.analyser.fftSize = 256;
                    
                    // Start media recorder with higher bitrate for better quality
                    const options = { mimeType: 'audio/webm;codecs=opus', audioBitsPerSecond: 128000 };
                    AppState.mediaRecorder = new MediaRecorder(stream, options);
                    
                    // Set up audio analysis for silence detection
                    AppState.source = AppState.audioContext.createMediaStreamSource(stream);
                    AppState.source.connect(AppState.analyser);
                    
                    // Start monitoring volume for silence detection
                    monitorVolume();
                    
                    // Add event listeners for audio data
                    AppState.mediaRecorder.addEventListener('dataavailable', event => {
                        if (event.data.size > 0) {
                            AppState.audioChunks.push(event.data);
                        }
                    });
                    
                    // When recording stops, process the audio
                    AppState.mediaRecorder.addEventListener('stop', () => {
                        console.log(`${AppState.logPrefix} MediaRecorder stopped, chunks: ${AppState.audioChunks.length}`);
                        if (AppState.audioChunks.length > 0) {
                            AppState.audioBlob = new Blob(AppState.audioChunks, { type: 'audio/webm;codecs=opus' });
                            console.log(`${AppState.logPrefix} Audio blob created, size: ${AppState.audioBlob.size}`);
                            if (!AppState.isProcessing) {
                                processAudio();
                            }
                        }
                    });
                    
                    // Start recording in chunks of 100ms
                    AppState.mediaRecorder.start(100);
                    statusText.textContent = "Listening...";
                    
                    // Start continuous recording timer if enabled
                    if (settings.continuousMode) {
                        startContinuousRecordingTimer();
                    }
                    
                    // Setup visualization animation
                    function animate() {
                        if (!AppState.isListening) return;
                        
                        const bufferLength = AppState.analyser.frequencyBinCount;
                        const dataArray = new Uint8Array(bufferLength);
                        AppState.analyser.getByteFrequencyData(dataArray);
                        
                        // Calculate average volume
                        let sum = 0;
                        for (let i = 0; i < bufferLength; i++) {
                            sum += dataArray[i];
                        }
                        const average = sum / bufferLength;
                        
                        // Scale particles based on volume
                        const particles = document.querySelectorAll('.particle');
                        const scale = 1 + (average / 256) * 0.5; // Scale between 1 and 1.5
                        
                        // Make cosmos sphere pulsate based on audio level
                        const pulseScale = 1 + (average / 256) * 0.1; // Subtle pulse effect
                        cosmosSphere.style.transform = `scale(${pulseScale})`;
                        
                        particles.forEach(particle => {
                            particle.style.transform = `scale(${scale})`;
                        });
                        
                        requestAnimationFrame(animate);
                    }
                    
                    animate();
                    
                } catch (err) {
                    console.error(`${AppState.logPrefix} Error accessing microphone:`, err);
                    alert('Error accessing microphone. Please ensure your microphone is connected and permissions are granted.');
                    
                    // Reset state on error
                    AppState.isRecording = false;
                    AppState.isListening = false;
                }
            }
            
            // Function to start continuous recording timer
            function startContinuousRecordingTimer() {
                // Clear any existing timer
                if (AppState.continuousRecordingTimer) {
                    clearInterval(AppState.continuousRecordingTimer);
                    AppState.continuousRecordingTimer = null;
                }
                
                console.log(`${AppState.logPrefix} Continuous listening mode active - will process on speech followed by silence`);
            }
            
            // Stop recording function
            function stopRecording() {
                console.log(`${AppState.logPrefix} Stopping recording`);
                
                // Only stop if we have an active recorder
                if (AppState.mediaRecorder && AppState.mediaRecorder.state !== 'inactive') {
                    AppState.mediaRecorder.stop();
                    AppState.mediaRecorder.stream.getTracks().forEach(track => track.stop());
                }
                
                AppState.isRecording = false;
                AppState.isListening = false;
                micButton.classList.remove('recording');
                
                // Clear continuous recording timer
                if (AppState.continuousRecordingTimer) {
                    clearInterval(AppState.continuousRecordingTimer);
                    AppState.continuousRecordingTimer = null;
                }
                
                stopListening();
            }
            
            async function processAudio() {
                try {
                    const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });
                    const formData = new FormData();
                    formData.append('audio', audioBlob);
                    
                    statusText.textContent = 'Processing...';
                    statusText.classList.add('active');
                    
                    // Send to backend for processing
                    const response = await fetch('/api/stt', {
                        method: 'POST',
                        body: formData
                    });
                    
                    const data = await response.json();

                    console.log('Backend response from stt:', data);
                    
                    if (data.success) {
                        // Process the transcript
                        const transcript = data.transcript;
                        console.log('Transcript:', transcript);
                        
                        // Show user message
                        const userBubble = document.createElement('div');
                        userBubble.className = 'user-bubble';
                        userBubble.textContent = transcript;
                        messageContainer.appendChild(userBubble);
                        messageContainer.style.display = 'block';
                        
                        statusText.textContent = 'Thinking...';
                        
                        // Now get AI response for the transcript
                        const aiResponse = await fetch('/api/tts', {
                            method: 'POST',
                            headers: {
                                'Content-Type': 'application/json'
                            },
                            body: JSON.stringify({ text: transcript })
                        });
                        
                        const aiData = await aiResponse.json();
                        
                        if (aiData.success) {
                            // Display AI response
                            simulateResponse(aiData.response);
                            
                            // Convert to speech
                            speakResponse(aiData.response);
                        } else {
                            simulateResponse("I'm sorry, I couldn't process your request. Please try again.");
                        }
                    } else {
                        simulateResponse("I couldn't hear you clearly. Could you please try again?");
                    }
                } catch (err) {
                    console.error('Error processing audio:', err);
                    simulateResponse("There was an error processing your audio. Please try again.");
                } finally {
                    isProcessing = false;
                    statusText.classList.remove('active');
                }
            }
            
            async function speakResponse(text) {
                try {
                    const response = await fetch('/api/tts', {
                        method: 'POST',
                        headers: {
                            'Content-Type': 'application/json'
                        },
                        body: JSON.stringify({ text })
                    });
                    
                    if (response.ok) {
                        const blob = await response.blob();
                        const url = URL.createObjectURL(blob);
                        const audio = new Audio(url);
                        audio.play();
                    }
                } catch (err) {
                    console.error('Error with text-to-speech:', err);
                }
            }
            
            // Monitor volume and detect silence
            function monitorVolume() {
                if (!AppState.analyser || !AppState.isRecording) {
                    console.log(`${AppState.logPrefix} Cannot monitor volume - analyser or recording state invalid`);
                    return;
                }
                
                const bufferLength = AppState.analyser.frequencyBinCount;
                const volumeData = new Uint8Array(bufferLength);
                AppState.analyser.getByteFrequencyData(volumeData);
                
                // Calculate average volume level
                let sum = 0;
                for (let i = 0; i < bufferLength; i++) {
                    sum += volumeData[i];
                }
                const avg = sum / bufferLength;
                
                // Check if we've detected significant speech
                if (avg > settings.significantAudioThreshold) {
                    if (!AppState.significantSpeechDetected) {
                        console.log(`${AppState.logPrefix} Speech detected - volume level: ${avg.toFixed(1)}`);
                    }
                    AppState.significantSpeechDetected = true;
                    // Reset silence start when speech is detected
                    AppState.silenceStart = null;
                }
                
                // Regular audio detection for visualization
                if (avg > settings.silenceThreshold) {
                    AppState.hasDetectedAudio = true;
                }
                
                // Silence detection logic - only trigger processing if we had significant speech followed by silence
                if (AppState.significantSpeechDetected) {
                    if (avg < settings.silenceThreshold) {
                        // Start silence timer if not already started
                        if (!AppState.silenceStart) {
                            AppState.silenceStart = Date.now();
                            console.log(`${AppState.logPrefix} Silence started after speech`);
                        }
                        
                        // Check if silence has lasted long enough to be a pause
                        const silenceDuration = (Date.now() - AppState.silenceStart) / 1000;
                        
                        // Only process when:
                        // 1. We've detected significant speech
                        // 2. Followed by silence for the pause threshold duration
                        // 3. We're not already processing
                        // 4. Enough time has passed since last processing
                        if (silenceDuration > settings.pauseThreshold && !AppState.pauseDetected && !AppState.isProcessing &&
                            (Date.now() - AppState.lastProcessedTime > 3000)) {
                            
                            console.log(`${AppState.logPrefix} Processing after ${silenceDuration.toFixed(1)}s of silence following speech`);
                            AppState.pauseDetected = true;
                            pauseAndProcess();
                        }
                    } else {
                        // Reset silence timer if sound detected
                        if (AppState.silenceStart) {
                            console.log(`${AppState.logPrefix} Silence interrupted - continuing to listen`);
                        }
                        AppState.silenceStart = null;
                    }
                }
                
                // Continue monitoring as long as we're recording
                if (AppState.isRecording) {
                    requestAnimationFrame(monitorVolume);
                } else {
                    console.log(`${AppState.logPrefix} Volume monitoring stopped - recording ended`);
                }
            }
            
            // Pause recording and process current audio
            function pauseAndProcess() {
                console.log(`${AppState.logPrefix} Pausing to process audio`);
                
                if (!AppState.isRecording || AppState.isProcessing) {
                    console.log(`${AppState.logPrefix} Cannot pause - not recording or already processing`);
                    return;
                }
                
                // Stop the current recorder to get the data
                const currentMediaRecorder = AppState.mediaRecorder;
                currentMediaRecorder.stop();
                
                statusText.textContent = "Processing your message...";
                
                // Update state
                AppState.isRecording = false;
                
                // Store the callback on the window object so it can be accessed from anywhere
                window.resumeRecordingCallback = resumeRecording;
            }
            
            // Resume recording function for continuous conversation
            const resumeRecording = async () => {
                console.log(`${AppState.logPrefix} Resuming recording callback triggered`);
                
                // Check if still in listening mode
                if (AppState.isListening) {
                    console.log(`${AppState.logPrefix} Resuming recording - still active`);
                    
                    try {
                        // Reset AppState for a fresh start with the new recording session
                        AppState.isRecording = false;
                        AppState.isProcessing = false;
                        AppState.significantSpeechDetected = false;
                        AppState.pauseDetected = false;
                        AppState.silenceStart = null;
                        AppState.audioChunks = [];
                        
                        // Request a fresh microphone stream
                        const constraints = {
                            audio: {
                                echoCancellation: settings.echoCancel,
                                noiseSuppression: settings.noiseSuppress,
                                autoGainControl: true
                            }
                        };
                        
                        console.log(`${AppState.logPrefix} Requesting fresh microphone stream`);
                        const stream = await navigator.mediaDevices.getUserMedia(constraints);
                        
                        // Start a new recorder with the fresh stream
                        const options = { mimeType: 'audio/webm;codecs=opus', audioBitsPerSecond: 128000 };
                        AppState.mediaRecorder = new MediaRecorder(stream, options);
                        
                        // Set up event handlers again
                        AppState.mediaRecorder.addEventListener('dataavailable', event => {
                            if (event.data.size > 0) {
                                AppState.audioChunks.push(event.data);
                            }
                        });
                        
                        AppState.mediaRecorder.addEventListener('stop', () => {
                            if (AppState.audioChunks.length > 0) {
                                AppState.audioBlob = new Blob(AppState.audioChunks, { type: 'audio/webm;codecs=opus' });
                                if (!AppState.isProcessing) {
                                    processAudio();
                                }
                            }
                        });
                        
                        // Set up audio analysis again with the fresh stream
                        if (AppState.audioContext && AppState.analyser) {
                            // Disconnect previous source if exists
                            if (AppState.source) {
                                AppState.source.disconnect();
                            }
                            
                            AppState.source = AppState.audioContext.createMediaStreamSource(stream);
                            AppState.source.connect(AppState.analyser);
                            
                            // Start recording first, then visualization
                            AppState.mediaRecorder.start(100);
                            AppState.isRecording = true;
                            
                            // Start monitoring volume with the fresh stream
                            setTimeout(() => {
                                console.log(`${AppState.logPrefix} Starting volume monitoring`);
                                monitorVolume();
                            }, 100);
                        } else {
                            throw new Error("Audio context or analyser not properly initialized");
                        }
                        
                        statusText.textContent = "Listening...";
                        console.log(`${AppState.logPrefix} Successfully restarted recording with fresh stream`);
                        
                    } catch (error) {
                        console.error(`${AppState.logPrefix} Error resuming recording:`, error);
                        statusText.textContent = "Error resuming: " + error.message;
                    }
                } else {
                    console.log(`${AppState.logPrefix} Not resuming recording - conversation has ended`);
                }
            };
            
            // Process audio function
            async function processAudio() {
                if (!AppState.audioBlob || AppState.audioBlob.size === 0 || AppState.isProcessing) {
                    console.log(`${AppState.logPrefix} Cannot process audio - no blob, empty blob, or already processing`);
                    return;
                }
                
                AppState.isProcessing = true;
                AppState.lastProcessedTime = Date.now();
                console.log(`${AppState.logPrefix} Processing audio blob of size: ${AppState.audioBlob.size} bytes`);
                
                // Skip processing if the audio is too small to contain meaningful speech
                if (AppState.audioBlob.size < settings.minAudioSize) {
                    console.log(`${AppState.logPrefix} Audio too small to contain speech, skipping processing`);
                    AppState.isProcessing = false;
                    
                    if (window.resumeRecordingCallback && settings.continuousMode) {
                        window.resumeRecordingCallback();
                    }
                    return;
                }
                
                // Create FormData with the audio blob
                const formData = new FormData();
                formData.append('audio', AppState.audioBlob);
                
                try {
                    statusText.textContent = 'Converting speech to text...';
                    console.log(`${AppState.logPrefix} Sending audio to STT API`);
                    
                    const response = await fetch('/api/stt', {
                        method: 'POST',
                        body: formData
                    });
                    
                    if (!response.ok) {
                        throw new Error(`STT server error: ${response.status}`);
                    }
                    
                    const data = await response.json();
                    if (!data.success) {
                        throw new Error(data.error || 'Failed to transcribe audio');
                    }
                    
                    const transcript = data.transcript;
                    console.log(`${AppState.logPrefix} Received transcript: "${transcript}"`);
                    
                    // If no transcript was detected, skip processing
                    if (!transcript || transcript.trim() === '') {
                        // Silently resume recording without any messages about no speech
                        console.log(`${AppState.logPrefix} No speech detected in transcription`);
                        AppState.isProcessing = false;
                        if (window.resumeRecordingCallback && settings.continuousMode) {
                            window.resumeRecordingCallback();
                        }
                        return;
                    }
                    
                    // Send transcript to AI for processing
                    statusText.textContent = 'Getting AI response...';
                    const aiResponse = await fetch('/api/agent', {
                        method: 'POST',
                        headers: {
                            'Content-Type': 'application/json'
                        },
                        body: JSON.stringify({
                            transcript: transcript,
                            system_prompt: 'You are a helpful assistant.'
                        })
                    });
                    
                    if (!aiResponse.ok) {
                        throw new Error(`Agent server error: ${aiResponse.status}`);
                    }
                    
                    const aiData = await aiResponse.json();
                    if (!aiData.success) {
                        throw new Error(aiData.error || 'Failed to get AI response');
                    }
                    
                    // Display and speak AI response
                    simulateResponse(aiData.response);
                    await speakResponse(aiData.response);
                    
                    // After processing is complete, resume recording if in continuous mode
                    if (settings.continuousMode && window.resumeRecordingCallback) {
                        window.resumeRecordingCallback();
                    }
                    
                } catch (error) {
                    console.error(`${AppState.logPrefix} Error:`, error);
                    statusText.textContent = 'Error: ' + error.message;
                    
                    // Resume listening after error
                    setTimeout(() => {
                        if (settings.continuousMode && window.resumeRecordingCallback) {
                            window.resumeRecordingCallback();
                        }
                    }, 2000);
                } finally {
                    AppState.isProcessing = false;
                    AppState.pauseDetected = false;
                }
            }
            
            async function speakResponse(text) {
                try {
                    const response = await fetch('/api/tts', {
                        method: 'POST',
                        headers: {
                            'Content-Type': 'application/json'
                        },
                        body: JSON.stringify({ text: text })
                    });
                    
                    if (!response.ok) {
                        throw new Error('Failed to convert text to speech');
                    }
                    
                    const blob = await response.blob();
                    const audioURL = URL.createObjectURL(blob);
                    const audio = new Audio(audioURL);
                    audio.play();
                } catch (error) {
                    console.error('TTS Error:', error);
                }
            }
            
            // Create cosmic particles in the sphere
            function createCosmicParticles() {
                const colors = ['white', 'blue', 'cyan', 'purple', 'pink'];
                const totalParticles = 200;
                const sphereRadius = 180; // Radius of the cosmic sphere
                
                cosmosSphere.innerHTML = ''; // Clear existing particles
                
                for (let i = 0; i < totalParticles; i++) {
                    const particle = document.createElement('div');
                    particle.className = `particle ${colors[Math.floor(Math.random() * colors.length)]}`;
                    
                    // Random size between 2-6px
                    const size = Math.random() * 4 + 2;
                    particle.style.width = `${size}px`;
                    particle.style.height = `${size}px`;
                    
                    // Position randomly within the sphere (spherical coordinates)
                    const theta = Math.random() * Math.PI * 2; // Horizontal angle
                    const phi = Math.acos(2 * Math.random() - 1); // Vertical angle
                    const radius = Math.random() * sphereRadius; // Distance from center
                    
                    const x = radius * Math.sin(phi) * Math.cos(theta);
                    const y = radius * Math.sin(phi) * Math.sin(theta);
                    const z = radius * Math.cos(phi);
                    
                    // Calculate opacity based on distance from center (more transparency at edges)
                    const opacity = 0.1 + (1 - (radius / sphereRadius)) * 0.9;
                    
                    // Set animation delay as a CSS variable
                    const delay = Math.random() * 2;
                    particle.style.setProperty('--particle-delay', delay);
                    
                    // Apply styles
                    particle.style.left = `calc(50% + ${x}px)`;
                    particle.style.top = `calc(50% + ${y}px)`;
                    particle.style.opacity = opacity;
                    particle.style.transform = `translateZ(${z}px)`;
                    
                    // Add subtle movement animation
                    const animDuration = 20 + Math.random() * 80;
                    const animDelay = Math.random() * -animDuration;
                    
                    // Use multiple animations for more interesting movement
                    particle.style.animation = `
                        rotate ${animDuration}s linear ${animDelay}s infinite,
                        float ${10 + Math.random() * 10}s ease-in-out ${Math.random() * -10}s infinite
                    `;
                    
                    cosmosSphere.appendChild(particle);
                }
            }
            
            // Start listening animation
            function startListening() {
                isListening = true;
                document.body.classList.add('listening');
                statusText.classList.add('active');
                statusText.textContent = 'Listening...';
                cosmosSphere.classList.add('pulsate');
                messageContainer.style.display = 'none';
                
                // Make particles bulge with microphone input (real-time audio will handle this)
                const particles = document.querySelectorAll('.particle');
                particles.forEach(particle => {
                    // Add random delay for each particle to create a wave effect
                    const randomDelay = Math.random() * 0.5;
                    particle.style.setProperty('--particle-delay', randomDelay);
                });
            }
            
            // Stop listening animation
            function stopListening() {
                isListening = false;
                document.body.classList.remove('listening');
                statusText.classList.remove('active');
                cosmosSphere.classList.remove('pulsate');
                cosmosSphere.style.transform = '';
                
                // Show welcome message after response is displayed
                setTimeout(() => {
                    const welcomeMessage = document.getElementById('welcomeMessage');
                    if (welcomeMessage && !messageContainer.style.display || messageContainer.style.display === 'none') {
                        welcomeMessage.style.opacity = '0.95';
                        welcomeMessage.style.transform = 'translateY(-100px)';
                    }
                }, 1000);
                
                // Reset particles
                const particles = document.querySelectorAll('.particle');
                particles.forEach(particle => {
                    particle.style.transform = '';
                });
            }
            
            // Display AI response with animation
            function simulateResponse(text) {
                // Display the message container
                messageContainer.style.display = 'block';
                
                // Create a new message bubble
                const aiBubble = document.createElement('div');
                aiBubble.className = 'ai-bubble';
                messageContainer.appendChild(aiBubble);
                
                // Animate text appearing word by word
                const words = text.split(' ');
                responseMessage.textContent = ''; // Clear existing text for the SR reader
                
                let wordIndex = 0;
                const wordInterval = setInterval(() => {
                    if (wordIndex < words.length) {
                        aiBubble.textContent += (wordIndex === 0 ? '' : ' ') + words[wordIndex];
                        responseMessage.textContent = aiBubble.textContent; // For screen readers
                        wordIndex++;
                    } else {
                        clearInterval(wordInterval);
                    }
                }, 50); // Speed of text appearance - a bit faster than before
                
                // Scroll to the bottom of the container
                messageContainer.scrollTop = messageContainer.scrollHeight;
            }
        });
    </script>

    <!-- Footer -->
    <footer class="py-4 text-center text-blue-800 mt-auto">
        <p class="text-sm">&copy; 2025 DelightMate - Your Intelligent Assistant</p>
    </footer>

</body>
</html>