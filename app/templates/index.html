<!DOCTYPE html>
<html lang="en" data-theme="dark">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>DelightMate - Your AI Assistant</title>
    <link href="{{ url_for('static', filename='css/tailwind.css') }}" rel="stylesheet">
    <link href="{{ url_for('static', filename='css/cosmic.css') }}" rel="stylesheet">
    <script src="{{ url_for('static', filename='js/theme.js') }}"></script>
    <style>
        :root {
            --bg-color: #121212;
            --text-color: #ffffff;
            --accent-color: #50b3a2;
            --secondary-color: #3a8292;
            --font-family: 'Inter', sans-serif;
            --sidebar-width: 350px; /* Define sidebar width as a variable for consistency */
            --main-container-width: 100%; /* Default width */
            --main-container-transform: 0; /* Default transform */
        }
        
        /* Light Mode Variables */
        [data-theme="light"] {
            --bg-color: #f5f5f5;
            --text-color: #121212;
        }
        
        body, html {
            margin: 0;
            padding: 0;
            height: 100%;
            overflow: hidden;
            font-family: 'Arial', sans-serif;
        }
        
        .cosmos-container {
            position: relative;
            display: flex;
            flex-direction: column;
            justify-content: center;
            align-items: center;
            min-height: 100vh;
            width: var(--main-container-width);
            transition: all 0.3s ease;
            transform: translateX(var(--main-container-transform));
            overflow-x: hidden; /* Prevent horizontal scrolling */
        }
        
        /* Chat container for messages */
        .conversation-container {
            display: flex;
            flex-direction: column;
            position: absolute;
            top: 55%; /* Position to appear between cosmic sphere and buttons */
            left: 50%;
            transform: translate(-50%, -50%);
            width: 80%;
            max-width: 500px; /* Reduced max-width for smaller chat window */
            background-color: rgba(0, 0, 0, 0.4);
            border-radius: 12px;
            padding: 10px;
            color: var(--text-color);
            backdrop-filter: blur(10px);
            max-height: 200px; /* Reduced max-height */
            overflow-y: auto;
            z-index: 5;
        }
        
        /* Message styling */
        .message {
            margin-bottom: 10px;
            display: flex;
            flex-direction: column;
            max-width: 80%;
        }
        
        .message.user {
            align-self: flex-end;
            text-align: right;
        }
        
        .message.ai {
            align-self: flex-start;
            text-align: left;
        }
        
        .message-content {
            padding: 8px 12px;
            border-radius: 12px;
            word-wrap: break-word;
        }
        
        .message.user .message-content {
            background-color: var(--accent-color);
            color: white;
            border-radius: 12px 12px 0 12px;
        }
        
        .message.ai .message-content {
            background-color: rgba(255, 255, 255, 0.1);
            color: var(--text-color);
            border-radius: 12px 12px 12px 0;
        }
    </style>
</head>
<body style="background-color: var(--bg-color);">
    <div class="app-wrapper" style="display: flex; height: 100vh; width: 100vw; overflow: hidden;">
        <!-- Main Content Area -->
        <div class="cosmos-container" style="flex: 1; position: relative; overflow: hidden;">
        {% include 'components/navbar.html' %}
        
        <!-- User Profile -->
        {% if user and user.is_logged_in %}
        <div class="cosmic-user-profile">
            {% if user.picture %}
            <img src="{{ user.picture }}" alt="Profile">
            {% else %}
            <div style="width: 40px; height: 40px; border-radius: 50%; background-color: rgba(64, 190, 255, 0.8); display: flex; justify-content: center; align-items: center; color: white; font-weight: bold;">
                {{ user.name[0] if user.name else 'U' }}
            </div>
            {% endif %}
            <span class="cosmic-user-name">{{ user.name if user.name else 'User' }}</span>
        </div>
        {% endif %}
        
        <!-- Voice Mode Indicator -->
        <!-- <div class="voice-mode">Voice mode</div> -->
        
        <!-- Welcome Message -->
        <!-- <div class="cosmic-welcome" id="welcomeMessage">
            <h2>I'm DelightMate</h2>
            <p>Your digital teammate ready to help with emails, calendar, and more.</p>
            <p class="welcome-hint">Click the microphone to start speaking</p>
        </div> -->

        <!-- Cosmic Sphere with adjusted positioning -->
        <div class="cosmos-sphere" id="cosmosSphere" style="margin-top: -30px;"></div>
        
        <!-- Status Text - positioned higher -->
        <div class="status-text" id="statusText" style="position: absolute; top: 20%; left: 50%; transform: translateX(-50%); font-size: 18px; color: var(--accent-color); z-index: 10;">Listening...</div>
        
        <!-- Conversation Container -->
        <div class="conversation-container" id="messageContainer" style="display: none; position: absolute; top: 40%; left: 50%; transform: translateX(-50%); width: 80%; max-width: 450px; max-height: 40%; overflow-y: auto; z-index: 10; padding: 15px; border-radius: 12px; background-color: rgba(0, 0, 0, 0.2); backdrop-filter: blur(10px);">
            <!-- Close button for conversation -->
            <button id="closeConversation" style="position: absolute; top: 5px; right: 5px; background: none; border: none; color: white; font-size: 16px; cursor: pointer; z-index: 11;">×</button>
            <!-- Messages will be added here dynamically -->
            <div id="responseMessage" style="position: absolute; left: -9999px; visibility: hidden;"><!-- Hidden element for screen readers --></div>
        </div>
        
        <!-- Control Buttons -->
        <div class="controls">
            <button class="control-btn">
                <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                    <path d="M22 16.92v3a2 2 0 0 1-2.18 2 19.79 19.79 0 0 1-8.63-3.07 19.5 19.5 0 0 1-6-6 19.79 19.79 0 0 1-3.07-8.67A2 2 0 0 1 4.11 2h3a2 2 0 0 1 2 1.72 12.84 12.84 0 0 0 .7 2.81 2 2 0 0 1-.45 2.11L8.09 9.91a16 16 0 0 0 6 6l1.27-1.27a2 2 0 0 1 2.11-.45 12.84 12.84 0 0 0 2.81.7A2 2 0 0 1 22 16.92z"></path>
                </svg>
            </button>
            <button class="control-btn microphone-btn" id="micButton">
                <svg xmlns="http://www.w3.org/2000/svg" width="28" height="28" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                    <path d="M12 1a3 3 0 0 0-3 3v8a3 3 0 0 0 6 0V4a3 3 0 0 0-3-3z"></path>
                    <path d="M19 10v2a7 7 0 0 1-14 0v-2"></path>
                    <line x1="12" y1="19" x2="12" y2="23"></line>
                    <line x1="8" y1="23" x2="16" y2="23"></line>
                </svg>
            </button>
            <button class="control-btn">
                <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                    <circle cx="11" cy="11" r="8"></circle>
                    <line x1="21" y1="21" x2="16.65" y2="16.65"></line>
                </svg>
            </button>
        </div>

    </div>
        
        <!-- Right Sidebar for Tools - with visible border in both themes -->
        <div id="toolSidebar" class="tool-sidebar" style="width: 0; height: 100vh; background-color: var(--bg-color); border-left: 2px solid var(--text-color); transition: width 0.3s ease; overflow: hidden;">
            <div class="tool-header" style="display: flex; justify-content: space-between; align-items: center; padding: 15px; border-bottom: 1px solid rgba(255, 255, 255, 0.1);">
                <h3 id="toolTitle" style="margin: 0; color: var(--text-color); font-size: 1.2rem;">Tool Title</h3>
                <button id="closeTool" style="background: none; border: none; color: var(--text-color); font-size: 20px; cursor: pointer;">×</button>
            </div>
            <div id="toolContent" class="tool-content" style="padding: 15px;">
                <!-- Tool content will be dynamically generated -->
            </div>
        </div>
    </div>

    <!-- JavaScript for Cosmic Visualization and Audio Recording -->
    <script>
        document.addEventListener('DOMContentLoaded', function() {
            const cosmosSphere = document.getElementById('cosmosSphere');
            const statusText = document.getElementById('statusText');
            const micButton = document.getElementById('micButton');
            const messageContainer = document.getElementById('messageContainer');
            const responseMessage = document.getElementById('responseMessage');
            
            // Create a dedicated state management object to centralize our application state
            // This will help us keep track of state across different function calls
            const AppState = {
                // Recording state
                isRecording: false,
                isProcessing: false,
                pauseDetected: false,
                silenceStart: null,
                hasDetectedAudio: false,
                significantSpeechDetected: false,

                // Audio resources
                mediaRecorder: null,
                audioChunks: [],
                audioBlob: null,
                audioContext: null,
                analyser: null,
                source: null,

                // Timers and tracking
                continuousRecordingTimer: null,
                lastProcessedTime: 0,

                // Conversation state
                conversationActive: false,

                // Tool state
                activeTool: null, // Current active tool
                pendingToolAction: null, // Action waiting for confirmation

                // Debug
                logPrefix: '🎙️ VoiceAssistant:',

                // Methods for state management
                startConversation() {
                    this.conversationActive = true;
                    console.log(`${this.logPrefix} Conversation started`);
                },

                endConversation() {
                    this.conversationActive = false;
                    this.resetAudioState();
                    console.log(`${this.logPrefix} Conversation ended`);
                },

                resetAudioState() {
                    this.isRecording = false;
                    this.isProcessing = false;
                    this.pauseDetected = false;
                    this.silenceStart = null;
                    this.hasDetectedAudio = false;
                    this.significantSpeechDetected = false;
                    this.audioChunks = [];
                    console.log(`${this.logPrefix} Audio state reset`);
                },

                activateTool(toolName) {
                    this.activeTool = toolName;
                    console.log(`${this.logPrefix} Tool activated: ${toolName}`);
                },

                deactivateTool() {
                    this.activeTool = null;
                    this.pendingToolAction = null;
                    console.log(`${this.logPrefix} Tool deactivated`);
                },

                debug() {
                    console.log(`${this.logPrefix} State: 
                        Recording: ${this.isRecording},
                        Processing: ${this.isProcessing}, 
                        Conversation: ${this.conversationActive},
                        ActiveTool: ${this.activeTool}
                    `);
                }
            };

            // Audio processing variables - using AppState now for most of these
            let volumeData = [];
            let pauseTimeout = null;

            // Settings with default values
            let settings = {
                silenceThreshold: 15,
                pauseThreshold: 2.0, // Silence pause threshold in seconds
                echoCancel: true,
                noiseSuppress: true,
                continuousMode: true,
                minAudioSize: 10000, // Minimum audio size to consider as valid speech (bytes)
                significantAudioThreshold: 25 // Threshold for significant audio (higher than silence threshold)
            };

            // Create cosmic particles
            createCosmicParticles();
            
            // Microphone button click handler
            micButton.addEventListener('click', function() {
                console.log(`${AppState.logPrefix} Mic button clicked`);
                
                if (AppState.isRecording) {
                    stopRecording();
                    // Change microphone icon to original icon when stopping
                    micButton.innerHTML = `
                        <svg xmlns="http://www.w3.org/2000/svg" width="28" height="28" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                            <path d="M12 1a3 3 0 0 0-3 3v8a3 3 0 0 0 6 0V4a3 3 0 0 0-3-3z"></path>
                            <path d="M19 10v2a7 7 0 0 1-14 0v-2"></path>
                            <line x1="12" y1="19" x2="12" y2="23"></line>
                            <line x1="8" y1="23" x2="16" y2="23"></line>
                        </svg>
                    `;
                } else {
                    startConversation();
                    // Change microphone icon to stop icon when recording
                    micButton.innerHTML = `
                        <svg xmlns="http://www.w3.org/2000/svg" width="28" height="28" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                            <rect x="6" y="6" width="12" height="12" rx="2" ry="2"></rect>
                        </svg>
                    `;
                }
            });

            // Start the conversation session
            function startConversation() {
                // Update state
                AppState.startConversation();

                // Initialize audio components
                initAudioContext();
                statusText.textContent = "Starting microphone...";

                // Begin recording
                startRecording();
            }

            // End the conversation session
            function endConversation() {
                console.log(`${AppState.logPrefix} Ending conversation`);

                // Stop current recording if active
                if (AppState.isRecording) {
                    stopRecording();
                }

                // Clear any pending timers
                if (pauseTimeout) {
                    clearTimeout(pauseTimeout);
                    pauseTimeout = null;
                }

                // Clear continuous recording timer
                if (AppState.continuousRecordingTimer) {
                    clearInterval(AppState.continuousRecordingTimer);
                    AppState.continuousRecordingTimer = null;
                }

                // Update application state
                AppState.endConversation();

                // Update UI
                statusText.textContent = "Ready to start...";
            }

            // Initialize AudioContext (for volume analysis)
            function initAudioContext() {
                try {
                    // Initialize or reset
                    if (AppState.audioContext) {
                        AppState.audioContext.close();
                    }

                    window.AudioContext = window.AudioContext || window.webkitAudioContext;
                    AppState.audioContext = new AudioContext();
                    AppState.analyser = AppState.audioContext.createAnalyser();
                    AppState.analyser.fftSize = 256;

                    console.log(`${AppState.logPrefix} Audio context initialized successfully`);
                } catch (e) {
                    console.error(`${AppState.logPrefix} AudioContext not supported or error initializing:`, e);
                    statusText.textContent = "Audio features not supported in your browser";
                }
            }

            // Start recording audio
            async function startRecording() {
                if (AppState.isRecording) {
                    console.log(`${AppState.logPrefix} Already recording, not starting again`);
                    return;
                }

                console.log(`${AppState.logPrefix} Starting recording...`);
                AppState.debug();

                // Reset state variables
                AppState.isRecording = true;
                AppState.pauseDetected = false;
                AppState.silenceStart = null;
                AppState.hasDetectedAudio = false;
                AppState.significantSpeechDetected = false;
                AppState.audioChunks = [];

                try {
                    // Make sure audio context is initialized
                    if (!AppState.audioContext || !AppState.analyser) {
                        console.log(`${AppState.logPrefix} Re-initializing audio context`);
                        initAudioContext();
                    }

                    // Create constraints with echo cancellation and noise suppression
                    const constraints = {
                        audio: {
                            echoCancellation: settings.echoCancel,
                            noiseSuppression: settings.noiseSuppress,
                            autoGainControl: true
                        }
                    };

                    statusText.textContent = "Accessing microphone...";
                    const stream = await navigator.mediaDevices.getUserMedia(constraints);

                    // Start media recorder with higher bitrate for better quality
                    const options = { mimeType: 'audio/webm;codecs=opus', audioBitsPerSecond: 128000 };
                    AppState.mediaRecorder = new MediaRecorder(stream, options);

                    // Set up audio analysis for pause detection
                    if (AppState.audioContext && AppState.analyser) {
                        // Disconnect previous source if exists
                        if (AppState.source) {
                            AppState.source.disconnect();
                        }

                        AppState.source = AppState.audioContext.createMediaStreamSource(stream);
                        AppState.source.connect(AppState.analyser);

                        // Start monitoring volume
                        monitorVolume();
                        
                        // Start animation
                        startListening();
                    } else {
                        throw new Error("Audio context or analyser not initialized");
                    }

                    AppState.mediaRecorder.addEventListener('dataavailable', event => {
                        if (event.data.size > 0) {
                            AppState.audioChunks.push(event.data);
                        }
                    });

                    AppState.mediaRecorder.addEventListener('stop', () => {
                        console.log(`${AppState.logPrefix} MediaRecorder stopped, chunks: ${AppState.audioChunks.length}`);
                        if (AppState.audioChunks.length > 0) {
                            AppState.audioBlob = new Blob(AppState.audioChunks, { type: 'audio/webm;codecs=opus' });
                            console.log(`${AppState.logPrefix} Audio blob created, size: ${AppState.audioBlob.size}`);
                            if (!AppState.isProcessing) {
                                processAudio();
                            }
                        }
                    });

                    AppState.mediaRecorder.start(100); // Collect data in 100ms chunks
                    micButton.classList.add('recording');
                    statusText.textContent = "Listening...";

                    // Start continuous recording timer if enabled
                    if (settings.continuousMode) {
                        startContinuousRecordingTimer();
                    }

                } catch (error) {
                    console.error(`${AppState.logPrefix} Error starting recording:`, error);
                    statusText.textContent = "Error starting recording: " + error.message;

                    // Reset state on error
                    AppState.isRecording = false;
                }
            }

            // New function to handle continuous recording
            function startContinuousRecordingTimer() {
                // Clear any existing timer
                if (AppState.continuousRecordingTimer) {
                    clearInterval(AppState.continuousRecordingTimer);
                    AppState.continuousRecordingTimer = null;
                }

                console.log(`${AppState.logPrefix} Continuous listening mode active - will process on speech followed by silence`);
            }

            // Stop recording
            function stopRecording() {
                console.log(`${AppState.logPrefix} Stopping recording`);

                // Only stop if we have an active recorder
                if (AppState.mediaRecorder && AppState.mediaRecorder.state !== 'inactive') {
                    AppState.mediaRecorder.stop();
                    AppState.mediaRecorder.stream.getTracks().forEach(track => track.stop());
                }

                AppState.isRecording = false;
                AppState.conversationActive = false;
                micButton.classList.remove('recording');

                // Clear continuous recording timer
                if (AppState.continuousRecordingTimer) {
                    clearInterval(AppState.continuousRecordingTimer);
                    AppState.continuousRecordingTimer = null;
                }
            }

            // Monitor volume and detect silence
            function monitorVolume() {
                if (!AppState.analyser || !AppState.isRecording) {
                    console.log(`${AppState.logPrefix} Cannot monitor volume - analyser or recording state invalid`);
                    return;
                }

                const bufferLength = AppState.analyser.frequencyBinCount;
                const volumeData = new Uint8Array(bufferLength);
                AppState.analyser.getByteFrequencyData(volumeData);

                // Calculate average volume level
                let sum = 0;
                for (let i = 0; i < bufferLength; i++) {
                    sum += volumeData[i];
                }
                const avg = sum / bufferLength;

                // Check if we've detected significant speech (using a higher threshold)
                if (avg > settings.significantAudioThreshold) {
                    if (!AppState.significantSpeechDetected) {
                        console.log(`${AppState.logPrefix} Speech detected - volume level: ${avg.toFixed(1)}`);
                    }
                    AppState.significantSpeechDetected = true;
                    // Reset silence start when speech is detected
                    AppState.silenceStart = null;

                    // Only log occasionally to reduce console spam
                    if (Math.random() < 0.02) {
                        console.log(`${AppState.logPrefix} Detected speech: ${avg.toFixed(2)}`);
                    }
                }

                // Regular audio detection for visualization
                if (avg > settings.silenceThreshold) {
                    AppState.hasDetectedAudio = true;
                }

                // Silence detection logic - only trigger processing if we had significant speech followed by silence
                if (AppState.significantSpeechDetected) {
                    if (avg < settings.silenceThreshold) {
                        // Start silence timer if not already started
                        if (!AppState.silenceStart) {
                            AppState.silenceStart = Date.now();
                            console.log(`${AppState.logPrefix} Silence started after speech`);
                        }

                        // Check if silence has lasted long enough to be a pause
                        const silenceDuration = (Date.now() - AppState.silenceStart) / 1000;

                        // Update debug status more frequently during silence
                        if (Math.random() < 0.1) {
                            console.log(`${AppState.logPrefix} Silence duration: ${silenceDuration.toFixed(1)}s`);
                        }

                        // Only process when:
                        // 1. We've detected significant speech
                        // 2. Followed by silence for the pause threshold duration
                        // 3. We're not already processing
                        // 4. Enough time has passed since last processing
                        if (silenceDuration > settings.pauseThreshold && !AppState.pauseDetected && !AppState.isProcessing &&
                            (Date.now() - AppState.lastProcessedTime > 3000)) {

                            console.log(`${AppState.logPrefix} Processing after ${silenceDuration.toFixed(1)}s of silence following speech`);
                            AppState.pauseDetected = true;
                            pauseAndProcess();
                        }
                    } else {
                        // Reset silence timer if sound detected
                        if (AppState.silenceStart) {
                            console.log(`${AppState.logPrefix} Silence interrupted - continuing to listen`);
                        }
                        AppState.silenceStart = null;
                    }
                }

                // Continue monitoring as long as we're recording
                if (AppState.isRecording) {
                    requestAnimationFrame(monitorVolume);
                } else {
                    console.log(`${AppState.logPrefix} Volume monitoring stopped - recording ended`);
                }
            }

            // Pause recording and process current audio
            function pauseAndProcess() {
                console.log(`${AppState.logPrefix} Pausing to process audio`);

                if (!AppState.isRecording || AppState.isProcessing) {
                    console.log(`${AppState.logPrefix} Cannot pause - not recording or already processing`);
                    return;
                }

                // Stop the current recorder to get the data
                const currentMediaRecorder = AppState.mediaRecorder;
                currentMediaRecorder.stop();

                statusText.textContent = "Processing your message...";

                // Update state
                AppState.isRecording = false;

                // Store the callback on the window object so it can be accessed from anywhere
                window.resumeRecordingCallback = resumeRecording;
            }

            // resumeRecording function - improved to fix visualization and silence detection
            const resumeRecording = async () => {
                console.log(`${AppState.logPrefix} Resuming recording callback triggered`);
                console.log(`${AppState.logPrefix} Conversation active: ${AppState.conversationActive}`);

                // Important: Use AppState.conversationActive to determine if we should resume
                if (AppState.conversationActive) {
                    console.log(`${AppState.logPrefix} Resuming recording - conversation is active`);

                    try {
                        // Reset AppState for a fresh start with the new recording session
                        AppState.isRecording = false;
                        AppState.isProcessing = false;
                        AppState.significantSpeechDetected = false;
                        AppState.pauseDetected = false;
                        AppState.silenceStart = null;
                        AppState.audioChunks = [];

                        // CRITICAL FIX: Instead of trying to reuse the old stream,
                        // request a fresh microphone stream each time
                        const constraints = {
                            audio: {
                                echoCancellation: settings.echoCancel,
                                noiseSuppression: settings.noiseSuppress,
                                autoGainControl: true
                            }
                        };

                        console.log(`${AppState.logPrefix} Requesting fresh microphone stream`);
                        const stream = await navigator.mediaDevices.getUserMedia(constraints);

                        // Reinitialize the audio context if needed
                        if (!AppState.audioContext || !AppState.analyser) {
                            console.log(`${AppState.logPrefix} Reinitializing audio context`);
                            initAudioContext();
                        }

                        // Start a new recorder with the fresh stream
                        const options = { mimeType: 'audio/webm;codecs=opus', audioBitsPerSecond: 128000 };
                        AppState.mediaRecorder = new MediaRecorder(stream, options);

                        // Set up event handlers again
                        AppState.mediaRecorder.addEventListener('dataavailable', event => {
                            if (event.data.size > 0) {
                                AppState.audioChunks.push(event.data);
                            }
                        });

                        AppState.mediaRecorder.addEventListener('stop', () => {
                            if (AppState.audioChunks.length > 0) {
                                AppState.audioBlob = new Blob(AppState.audioChunks, { type: 'audio/webm;codecs=opus' });
                                if (!AppState.isProcessing) {
                                    processAudio();
                                }
                            }
                        });

                        // Set up audio analysis again with the fresh stream
                        if (AppState.audioContext && AppState.analyser) {
                            // Disconnect previous source if exists
                            if (AppState.source) {
                                AppState.source.disconnect();
                            }

                            AppState.source = AppState.audioContext.createMediaStreamSource(stream);
                            AppState.source.connect(AppState.analyser);

                            // Start recording first, then visualization
                            AppState.mediaRecorder.start(100);
                            AppState.isRecording = true;

                            // Start monitoring volume with the fresh stream
                            // Using setTimeout to ensure the recorder is fully started
                            setTimeout(() => {
                                console.log(`${AppState.logPrefix} Starting volume monitoring`);
                                monitorVolume();
                            }, 100);
                        } else {
                            throw new Error("Audio context or analyser not properly initialized");
                        }

                        micButton.classList.add('recording');
                        statusText.textContent = "Listening...";

                        console.log(`${AppState.logPrefix} Successfully restarted recording with fresh stream`);

                        // Debug state
                        AppState.debug();

                    } catch (error) {
                        console.error(`${AppState.logPrefix} Error resuming recording:`, error);
                        statusText.textContent = "Error resuming: " + error.message;

                        // Try to fully restart recording as fallback
                        console.log(`${AppState.logPrefix} Falling back to complete restart`);
                        setTimeout(startRecording, 1000);
                    }
                } else {
                    console.log(`${AppState.logPrefix} Not resuming recording - conversation has ended`);
                }
            };

            // Process recorded audio - UPDATED VERSION
            async function processAudio() {
                if (!AppState.audioBlob || AppState.audioBlob.size === 0 || AppState.isProcessing) {
                    console.log(`${AppState.logPrefix} Cannot process audio - no blob, empty blob, or already processing`);
                    return;
                }

                AppState.isProcessing = true;
                AppState.lastProcessedTime = Date.now();
                console.log(`${AppState.logPrefix} Processing audio blob of size: ${AppState.audioBlob.size} bytes`);

                // Skip processing if the audio is too small to contain meaningful speech
                if (AppState.audioBlob.size < settings.minAudioSize) {
                    console.log(`${AppState.logPrefix} Audio too small to contain speech, skipping processing`);
                    AppState.isProcessing = false;

                    resumeRecordingAfterProcessing();
                    return;
                }

                // Create FormData with the audio blob
                const formData = new FormData();
                formData.append('audio', AppState.audioBlob);

                try {
                    // Step 1: Speech to Text
                    statusText.textContent = "Converting speech to text...";
                    console.log(`${AppState.logPrefix} Sending audio to STT API`);

                    const sttResponse = await fetch('/api/stt', {
                        method: 'POST',
                        body: formData,
                        // Add timeout to prevent hanging requests
                        signal: AbortSignal.timeout(10000) // 10 second timeout
                    });

                    if (!sttResponse.ok) {
                        throw new Error(`STT server error: ${sttResponse.status} - ${await sttResponse.text()}`);
                    }

                    const sttData = await sttResponse.json();
                    if (!sttData.success) {
                        throw new Error(sttData.error || 'Failed to transcribe audio');
                    }

                    const transcript = sttData.transcript;
                    console.log(`${AppState.logPrefix} Received transcript: "${transcript}"`);

                    if (!transcript || transcript.trim() === '') {
                        // Silently resume recording without any messages about no speech
                        console.log(`${AppState.logPrefix} No speech detected in transcription`);
                        AppState.isProcessing = false;
                        resumeRecordingAfterProcessing();
                        return;
                    }

                    // Skip if transcript contains only filler words
                    const fillerWords = ['um', 'uh', 'mmm', 'hmm', 'ah', 'er'];
                    const cleanTranscript = transcript.toLowerCase().trim();
                    if (fillerWords.some(word => cleanTranscript === word)) {
                        console.log(`${AppState.logPrefix} Skipping filler: "${cleanTranscript}"`);
                        AppState.isProcessing = false;
                        resumeRecordingAfterProcessing();
                        return;
                    }

                    // Add user message to conversation log
                    addToConversationLog(transcript, "user");

                    // Check for pending tool confirmations first
                    if (AppState.pendingToolAction) {
                        try {
                            const wasConfirmation = await processToolConfirmation(transcript);
                            if (wasConfirmation) {
                                // If it was a confirmation, we don't need to process further
                                AppState.isProcessing = false;
                                resumeRecordingAfterProcessing();
                                return;
                            }
                        } catch (error) {
                            console.error(`${AppState.logPrefix} Error processing tool confirmation:`, error);
                        }
                    }

                    // Step 2: Send to Agent with proper system prompt about capabilities
                    statusText.textContent = "Getting AI response...";
                    console.log(`${AppState.logPrefix} Sending transcript to Agent API with capabilities context`);

                    // Create a detailed system prompt that explains the assistant's capabilities
                    const systemPrompt = `You are DelightMate, a powerful voice assistant with these capabilities:

✅ Writing and sending emails
✅ Summarizing emails 
✅ Checking calendar availability
✅ Adding events to calendars
✅ Voice conversations

CAPABILITIES GUIDE:
1. When a user asks you to send an email, respond enthusiastically like "Of course! I'll help you draft that email." NEVER say you can't send emails.
2. When a user asks about their calendar, respond positively like "Let me check your calendar for you." NEVER say you can't access calendars.
3. Keep responses concise and conversational.

INTENT DETECTION:
When you identify a user intent related to a specific tool, include it in your response in this format:
- For email composition: {"tool": "email", "action": "compose", "params": {"recipient": "NAME", "subject": "SUBJECT"}}
- For email summary: {"tool": "email", "action": "summarize", "params": {"count": NUMBER}}
- For calendar checking: {"tool": "calendar", "action": "check", "params": {"date": "DATE"}}
- For calendar event creation: {"tool": "calendar", "action": "add", "params": {"title": "TITLE", "date": "DATE", "time": "TIME"}}
If no specific tool intent is detected, return null for the intent.

RESPONSE FORMAT:
- For regular questions: Provide a helpful, brief response.
- For email requests: Respond with "I'll help you write that email" or similar and assume you can do it.
- For calendar requests: Respond with "I'll check your calendar" or similar and assume you can do it.
- NEVER say you're an AI that can't perform these tasks - you are a voice assistant with these capabilities.

Your response should include both a conversational text reply AND the detected intent object.

The system will handle the actual implementation of these capabilities.`;

                    // Include the transcript and system prompt in the request to OpenAI
                    const agentRequestBody = {
                        transcript: transcript,
                        system_prompt: systemPrompt
                    };

                    // Send request to the agent API
                    const agentResponse = await fetch('/api/agent', {
                        method: 'POST',
                        headers: {
                            'Content-Type': 'application/json'
                        },
                        body: JSON.stringify(agentRequestBody),
                        // Add timeout to prevent hanging requests
                        signal: AbortSignal.timeout(30000) // 30 second timeout
                    });

                    if (!agentResponse.ok) {
                        throw new Error(`Agent server error: ${agentResponse.status}`);
                    }

                    const agentData = await agentResponse.json();
                    if (!agentData.success) {
                        throw new Error(agentData.error || 'Failed to process with agent');
                    }

                    console.log(`${AppState.logPrefix} Received agent response:`, agentData);

                    // Extract text response and detected intent from the agent response
                    const responseText = agentData.response;
                    const detectedIntent = agentData.intent;

                    // Add AI response to conversation log
                    addToConversationLog(responseText, "ai");

                    // Convert to speech using TTS API
                    await processAIResponseTTS(responseText);

                    // After the AI response and TTS, handle the detected tool intent if any
                    if (detectedIntent) {
                        try {
                            console.log(`${AppState.logPrefix} Processing detected intent from AI:`, detectedIntent);
                            AppState.isProcessingTool = true;
                            await handleToolIntent(detectedIntent, transcript);
                            AppState.isProcessingTool = false;
                        } catch (error) {
                            console.error(`${AppState.logPrefix} Error handling tool:`, error);
                            AppState.isProcessingTool = false;
                        }
                    }

                    // Resume recording after processing is complete
                    resumeRecordingAfterProcessing();

                } catch (error) {
                    console.error(`${AppState.logPrefix} Error:`, error);
                    statusText.textContent = 'Error: ' + error.message;
                    addToConversationLog("Sorry, I encountered an error: " + error.message, "ai");

                    // Resume listening after error
                    setTimeout(() => {
                        resumeRecordingAfterProcessing();
                    }, 2000);
                } finally {
                    AppState.isProcessing = false;
                    AppState.pauseDetected = false;
                }
            }

            // Helper function to resume recording after processing
            function resumeRecordingAfterProcessing() {
                if (window.resumeRecordingCallback) {
                    console.log(`${AppState.logPrefix} Calling resume callback after processing`);
                    const callback = window.resumeRecordingCallback;
                    window.resumeRecordingCallback = null;
                    callback();
                } else {
                    // Fallback if callback is missing - restart recording directly
                    console.log(`${AppState.logPrefix} No callback found - using fallback restart`);
                    if (AppState.conversationActive) {
                        setTimeout(startRecording, 500);
                    }
                }
            }

            // Helper function specifically for TTS without conversation log (already added by processAIResponse)
            async function processAIResponseTTS(message) {
                try {
                    statusText.textContent = "Converting response to speech...";
                    console.log(`${AppState.logPrefix} Sending response to TTS API`);

                    const ttsResponse = await fetch('/api/tts', {
                        method: 'POST',
                        headers: {
                            'Content-Type': 'application/json'
                        },
                        body: JSON.stringify({ text: message }),
                        // Add timeout to prevent hanging requests
                        signal: AbortSignal.timeout(15000) // 15 second timeout
                    });

                    if (!ttsResponse.ok) {
                        throw new Error(`TTS server error: ${ttsResponse.status}`);
                    }

                    console.log(`${AppState.logPrefix} Received TTS audio response`);

                    // Play the audio response
                    const audioBlob = await ttsResponse.blob();
                    const audioUrl = URL.createObjectURL(audioBlob);
                    const audioPlayer = new Audio(audioUrl);

                    // Create a Promise that resolves when audio playback completes
                    return new Promise((resolve, reject) => {
                        const onEnded = () => {
                            console.log(`${AppState.logPrefix} Audio playback ended, resuming listening`);
                            audioPlayer.removeEventListener('ended', onEnded);
                            audioPlayer.removeEventListener('error', onError);
                            resolve(true);
                        };

                        const onError = (error) => {
                            console.error(`${AppState.logPrefix} Audio playback error:`, error);
                            audioPlayer.removeEventListener('ended', onEnded);
                            audioPlayer.removeEventListener('error', onError);
                            reject(error);
                        };

                        // Set up event listeners
                        audioPlayer.addEventListener('ended', onEnded);
                        audioPlayer.addEventListener('error', onError);

                        statusText.textContent = "Playing response...";

                        // Start playback
                        audioPlayer.play().catch(error => {
                            console.error(`${AppState.logPrefix} Error starting audio playback:`, error);
                            onError(error);
                        });
                    });
                } catch (error) {
                    console.error(`${AppState.logPrefix} Error during TTS:`, error);
                    return false;
                }
            }

            // Add close conversation button functionality
            document.getElementById('closeConversation').addEventListener('click', () => {
                messageContainer.style.display = 'none';
                // Clear all messages
                const messages = messageContainer.querySelectorAll('.message');
                messages.forEach(msg => msg.remove());
            });

            // Add message to conversation log function
            function addToConversationLog(message, sender) {
                // Show conversation container if hidden
                messageContainer.style.display = 'block';
                
                const messageDiv = document.createElement('div');
                messageDiv.className = `message ${sender}`;
                
                const messageContent = document.createElement('div');
                messageContent.className = 'message-content';
                messageContent.textContent = message;
                
                messageDiv.appendChild(messageContent);
                messageContainer.appendChild(messageDiv);

                // Scroll to bottom
                messageContainer.scrollTop = messageContainer.scrollHeight;
            }

            // Tool detection system - analyzes transcript to identify tool intents
            function detectToolIntent(transcript) {
                const text = transcript.toLowerCase();
                console.log(`${AppState.logPrefix} Analyzing intent for: "${text}"`);

                // Email writing/sending detection
                if (text.includes("write an email") ||
                    text.includes("send an email") ||
                    text.includes("compose an email") ||
                    ((text.includes("email") || text.includes("mail")) &&
                        (text.includes("to ") || text.includes("write") || text.includes("send") || text.includes("compose")))) {

                    // Try to extract recipient
                    let recipient = "recipient";
                    let subject = "Meeting Follow-up";

                    console.log(`${AppState.logPrefix} Detected email intent with recipient: ${recipient}, subject: ${subject}`);

                    return {
                        tool: "email",
                        action: "compose",
                        params: { recipient, subject }
                    };
                }

                // Email summary detection
                if ((text.includes("summarize") || text.includes("summary") || text.includes("check")) &&
                    (text.includes("email") || text.includes("inbox") || text.includes("messages"))) {

                    // Try to extract count
                    let count = 3; // Default

                    console.log(`${AppState.logPrefix} Detected email summary intent for ${count} emails`);

                    return {
                        tool: "email",
                        action: "summarize",
                        params: { count }
                    };
                }

                // Calendar check detection
                if (text.includes("calendar") &&
                    (text.includes("check") || text.includes("look") || text.includes("view") ||
                        text.includes("availability") || text.includes("free") || text.includes("busy") ||
                        text.includes("schedule") || text.includes("appointments") ||
                        text.includes("when am i"))) {

                    // Try to extract date
                    let date = "today";

                    console.log(`${AppState.logPrefix} Detected calendar check intent for date: ${date}`);

                    return {
                        tool: "calendar",
                        action: "check",
                        params: { date }
                    };
                }

                // No tool intent detected
                console.log(`${AppState.logPrefix} No tool intent detected, treating as conversation`);
                return null;
            }

            // Handle tool activation based on detected intent
            async function handleToolIntent(toolIntent, transcript) {
                console.log(`${AppState.logPrefix} Handling tool intent:`, toolIntent);

                if (!toolIntent) return false;

                try {
                    switch (toolIntent.tool) {
                        case "email":
                            if (toolIntent.action === "compose") {
                                await handleEmailCompose(toolIntent.params, transcript);
                                return true;
                            } else if (toolIntent.action === "summarize") {
                                await handleEmailSummarize(toolIntent.params, transcript);
                                return true;
                            }
                            break;

                        case "calendar":
                            if (toolIntent.action === "check") {
                                await handleCalendarCheck(toolIntent.params, transcript);
                                return true;
                            } else if (toolIntent.action === "add") {
                                await handleCalendarAdd(toolIntent.params, transcript);
                                return true;
                            }
                            break;
                    }

                    return false;
                } catch (error) {
                    console.error(`${AppState.logPrefix} Error handling tool intent:`, error);
                    addToConversationLog(`I'm sorry, I encountered an error while trying to ${toolIntent.action} ${toolIntent.tool}. Please try again.`, "ai");
                    return true; // Return true to prevent normal conversation flow
                }
            }

            // Process user confirmation for pending actions
            async function processToolConfirmation(transcript) {
                if (!AppState.pendingToolAction) {
                    return false;
                }

                const affirmativeResponses = ['yes', 'yeah', 'sure', 'okay', 'ok', 'yep', 'correct', 'confirm', 'do it', 'please do', 'send it', 'that\'s right', 'sounds good', 'go ahead', 'definitely'];
                const negativeResponses = ['no', 'nope', 'cancel', 'don\'t', 'stop', 'wait', 'hold on', 'incorrect', 'that\'s wrong', 'nevermind', 'forget it', 'don\'t do that'];

                const cleanTranscript = transcript.toLowerCase().trim();

                // Check if the response is affirmative
                if (affirmativeResponses.some(response => cleanTranscript.includes(response))) {
                    console.log(`${AppState.logPrefix} Detected affirmative response: "${cleanTranscript}"`);
                    await executeToolAction(AppState.pendingToolAction, true);
                    AppState.pendingToolAction = null;
                    return true;
                }

                // Check if the response is negative
                if (negativeResponses.some(response => cleanTranscript.includes(response))) {
                    console.log(`${AppState.logPrefix} Detected negative response: "${cleanTranscript}"`);
                    addToConversationLog("I've canceled that action. Is there anything else you'd like me to do?", "ai");
                    hideTool();
                    AppState.pendingToolAction = null;
                    return true;
                }

                // If the response doesn't match confirmation patterns, return false
                return false;
            }

            // Execute the confirmed tool action
            async function executeToolAction(toolAction, isConfirmed) {
                if (!isConfirmed || !toolAction) {
                    return;
                }

                console.log(`${AppState.logPrefix} Executing tool action:`, toolAction);

                try {
                    switch (toolAction.tool) {
                        case "email":
                            if (toolAction.action === "send") {
                                // Brief status update only
                                statusText.textContent = "Sending email...";

                                // Simulate a brief waiting period
                                await new Promise(resolve => setTimeout(resolve, 1500));

                                // Concise confirmation
                                addToConversationLog(`Email sent to ${toolAction.params.to}.`, "ai");
                                hideTool();

                                // Deactivate the tool
                                AppState.deactivateTool();
                            }
                            break;

                        case "calendar":
                            if (toolAction.action === "create") {
                                const title = toolAction.params.title;
                                const date = toolAction.params.date;

                                // Update the status text only (no TTS)
                                statusText.textContent = "Creating calendar event...";

                                // Simulate a brief waiting period
                                await new Promise(resolve => setTimeout(resolve, 1500));

                                // Concise confirmation with TTS
                                addToConversationLog(`Event "${title}" added to calendar.`, "ai");
                                hideTool();

                                // Deactivate the tool
                                AppState.deactivateTool();
                            }
                            break;
                    }
                } catch (error) {
                    console.error(`${AppState.logPrefix} Error executing tool action:`, error);
                    addToConversationLog("Action failed. Please try again.", "ai");
                    hideTool();

                    // Make sure to deactivate the tool on error
                    AppState.deactivateTool();
                }
            }

            // Placeholder tool handlers - you can implement these as needed
            async function handleEmailCompose(params, transcript) {
                console.log(`${AppState.logPrefix} Handling email compose tool`);
                showTool("Email");
                // For now, just show a simple message
                document.getElementById('toolContent').innerHTML = `
                    <p>Email composition tool would appear here.</p>
                    <p>Recipient: ${params.recipient}</p>
                    <p>Subject: ${params.subject}</p>
                `;
            }

            async function handleEmailSummarize(params, transcript) {
                console.log(`${AppState.logPrefix} Handling email summarize tool`);
                showTool("Email Summary");
                document.getElementById('toolContent').innerHTML = `
                    <p>Email summary for ${params.count} recent emails would appear here.</p>
                `;
            }

            async function handleCalendarCheck(params, transcript) {
                console.log(`${AppState.logPrefix} Handling calendar check tool`);
                showTool("Calendar");
                document.getElementById('toolContent').innerHTML = `
                    <p>Calendar for ${params.date} would appear here.</p>
                `;
            }

            async function handleCalendarAdd(params, transcript) {
                console.log(`${AppState.logPrefix} Handling calendar add tool`);
                showTool("Calendar Event");
                document.getElementById('toolContent').innerHTML = `
                    <p>Add calendar event form would appear here.</p>
                    <p>Title: ${params.title}</p>
                    <p>Date: ${params.date}</p>
                    <p>Time: ${params.time}</p>
                `;
            }

            // Tool UI functions
            function showTool(title) {
                const toolSidebar = document.getElementById('toolSidebar');
                const toolTitle = document.getElementById('toolTitle');

                if (toolSidebar && toolTitle) {
                    toolTitle.textContent = title || 'Tool';
                    toolSidebar.style.width = '400px';
                    document.getElementById('toolContent').innerHTML = '';
                } else {
                    console.error(`${AppState.logPrefix} Tool UI elements not found`);
                }
            }

            function hideTool() {
                const toolSidebar = document.getElementById('toolSidebar');

                if (toolSidebar) {
                    toolSidebar.style.width = '0';
                    document.getElementById('toolContent').innerHTML = '';
                } else {
                    console.error(`${AppState.logPrefix} Tool UI element not found`);
                }
            }

            // Close tool button event listener
            document.getElementById('closeTool').addEventListener('click', () => {
                hideTool();
            });

            // Initialize on load
            AppState.isProcessingTool = false;
            console.log(`${AppState.logPrefix} Voice Assistant initialized on homepage`);

            // Create cosmic particles in the sphere
            function createCosmicParticles() {
                const colors = ['white', 'blue', 'cyan', 'purple', 'pink'];
                const totalParticles = 200;
                const sphereRadius = 180; // Radius of the cosmic sphere
                
                cosmosSphere.innerHTML = ''; // Clear existing particles
                
                for (let i = 0; i < totalParticles; i++) {
                    const particle = document.createElement('div');
                    particle.className = `particle ${colors[Math.floor(Math.random() * colors.length)]}`;
                    
                    // Random size between 2-6px
                    const size = Math.random() * 4 + 2;
                    particle.style.width = `${size}px`;
                    particle.style.height = `${size}px`;
                    
                    // Position randomly within the sphere (spherical coordinates)
                    const theta = Math.random() * Math.PI * 2; // Horizontal angle
                    const phi = Math.acos(2 * Math.random() - 1); // Vertical angle
                    const radius = Math.random() * sphereRadius; // Distance from center
                    
                    const x = radius * Math.sin(phi) * Math.cos(theta);
                    const y = radius * Math.sin(phi) * Math.sin(theta);
                    const z = radius * Math.cos(phi);
                    
                    // Calculate opacity based on distance from center (more transparency at edges)
                    const opacity = 0.1 + (1 - (radius / sphereRadius)) * 0.9;
                    
                    // Set animation delay as a CSS variable
                    const delay = Math.random() * 2;
                    particle.style.setProperty('--particle-delay', delay);
                    
                    // Apply styles
                    particle.style.left = `calc(50% + ${x}px)`;
                    particle.style.top = `calc(50% + ${y}px)`;
                    particle.style.opacity = opacity;
                    particle.style.transform = `translateZ(${z}px)`;
                    
                    // Add subtle movement animation
                    const animDuration = 20 + Math.random() * 80;
                    const animDelay = Math.random() * -animDuration;
                    
                    // Use multiple animations for more interesting movement
                    particle.style.animation = `
                        rotate ${animDuration}s linear ${animDelay}s infinite,
                        float ${10 + Math.random() * 10}s ease-in-out ${Math.random() * -10}s infinite
                    `;
                    
                    cosmosSphere.appendChild(particle);
                }
            }
            
            // Start listening animation
            function startListening() {
                document.body.classList.add('listening');
                statusText.classList.add('active');
                cosmosSphere.classList.add('pulsate');
                
                // Make particles bulge with microphone input (real-time audio will handle this)
                const particles = document.querySelectorAll('.particle');
                particles.forEach(particle => {
                    // Add random delay for each particle to create a wave effect
                    const randomDelay = Math.random() * 0.5;
                    particle.style.setProperty('--particle-delay', randomDelay);
                });

                // Setup visualization animation
                function animate() {
                    if (!AppState.isRecording) return;
                    
                    const bufferLength = AppState.analyser.frequencyBinCount;
                    const dataArray = new Uint8Array(bufferLength);
                    AppState.analyser.getByteFrequencyData(dataArray);
                    
                    // Calculate average volume
                    let sum = 0;
                    for (let i = 0; i < bufferLength; i++) {
                        sum += dataArray[i];
                    }
                    const average = sum / bufferLength;
                    
                    // Scale particles based on volume
                    const particles = document.querySelectorAll('.particle');
                    const scale = 1 + (average / 256) * 0.5; // Scale between 1 and 1.5
                    
                    // Make cosmos sphere pulsate based on audio level
                    const pulseScale = 1 + (average / 256) * 0.1; // Subtle pulse effect
                    cosmosSphere.style.transform = `scale(${pulseScale})`;
                    
                    particles.forEach(particle => {
                        particle.style.transform = `scale(${scale})`;
                    });
                    
                    requestAnimationFrame(animate);
                }
                
                animate();
            }
            
            // Stop listening animation
            function stopListening() {
                document.body.classList.remove('listening');
                statusText.classList.remove('active');
                cosmosSphere.classList.remove('pulsate');
                cosmosSphere.style.transform = '';
                
                // Reset particles
                const particles = document.querySelectorAll('.particle');
                particles.forEach(particle => {
                    particle.style.transform = '';
                });
            }
        });
    </script>

    <!-- Footer -->
    <footer class="py-4 text-center text-blue-800 mt-auto">
        <p class="text-sm">&copy; 2025 DelightMate - Your Intelligent Assistant</p>
    </footer>

</body>
</html>